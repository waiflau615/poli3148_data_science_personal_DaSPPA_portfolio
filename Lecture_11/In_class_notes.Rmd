---
title: "Lecture 11 in class notes (start from third section in text mining)"
author: "Lau Wai Fung Raymond"
date: "2023-11-23"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 3_Sentiment Analysis

## Introduction

This notebook demonstrate Sentiment Analsyis

```{r}
library(tidyverse)
library(lubridate)
```

```{r}
d_fulltext = read_rds("Lecture_10/1_Text_Mining/data/data_ce_speech_article.rds") # Change the date variable to "date" format d_fulltext = d_fulltext |> mutate(date_of_speech = dmy(date_of_speech))
```

## Tokenization

```{r}
if (!require("tidytext")) install.packages("tidytext")

library(tidytext) # Full introduction: http://tidytextmining.com/
```

```{r}
d_tokenized = d_fulltext |>
  select(uid, date_of_speech, text) |>
  unnest_tokens(word, text)

head(d_tokenized, 20)
```

## Wrangling: Remove Stop Words

```{r}
# Load Stopwords
data("stop_words")

head(stop_words, 20)
```

```{r}
# Remove stopwords
d_tokenized_s = d_tokenized |>
  anti_join(stop_words, by = "word")
# anti_join: whatever appearing in the stop_words dataframe, we remove it.
```

## Wrangling [Optional]: Stemming

```{r}
if (!require(SnowballC)) install.packages("SnowballC")
library(SnowballC)
```

```{r}
d_tokenized_s = d_tokenized_s |>
  mutate(stem = wordStem(word))

head(d_tokenized_s, 20)
```

## Sentiment Analysis

```{r}
if (!require(textdata)) install.packages("textdata")
library(textdata)
```

### Load Sentiment Dictionary

```{r}
dict_afinn = get_sentiments("afinn")
dict_bing = get_sentiments("bing")
dict_nrc = get_sentiments("nrc") 

table(dict_afinn$value)
table(dict_bing$sentiment)
table(dict_nrc$sentiment)

# Learn more https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm
```

Note, if you run this function for the first time, you will get a prompt in the console asking you to confirm that you are willing to download the sentiment dataset. The prompt looks as follows. Type "1" to install the dictionaries.

### Calculate the Simplest Sentiment Scores

```{r}
# Merge your tokenized documents with the sentiment dictionary
d_tokenized_s_afinn = d_tokenized_s |>
  select(uid, date_of_speech, word) |>
  inner_join(dict_afinn, by = "word")

# Aggregate the sentiment score for each document
d_tokenized_s_afinn_agg = d_tokenized_s_afinn |>
  group_by(uid, date_of_speech) |>
  summarise(sentiment_score = sum(value))

d_tokenized_s_afinn_agg = d_fulltext |>
  select(uid, title) |>
  left_join(d_tokenized_s_afinn_agg) |>
  mutate(sentiment_score = replace_na(sentiment_score, 0))

# Change of sentiment over time?
d_tokenized_s_afinn_agg |>
  ggplot(aes(x = date_of_speech, y = sentiment_score)) +
  geom_point(alpha = 0.6) +
  geom_smooth() +
  labs(
    title = "Sentiment Scores of Hong Kong CE's Speeches and Articles"
  ) +
  xlab("Date") + ylab("Sentiment Scores")
```

```{r}
# To do it better, we can normalize the sentiment scores by document lengths

# Merge your tokenized documents with the sentiment dictionary
d_tokenized_s_afinn = d_tokenized_s |>
  group_by(uid) |> mutate(doc_length = n()) |>
  ungroup() |>
  select(uid, date_of_speech, word, doc_length) |>
  inner_join(dict_afinn, by = "word") |>
  ungroup()

# Aggregate the sentiment score for each document
d_tokenized_s_afinn_agg = d_tokenized_s_afinn |>
  group_by(uid, date_of_speech) |>
  summarise(sentiment_score = sum(value) / mean(doc_length))

d_tokenized_s_afinn_agg = d_fulltext |>
  select(uid) |>
  left_join(d_tokenized_s_afinn_agg) |>
  mutate(sentiment_score = replace_na(sentiment_score, 0))

# Change of sentiment over time?
d_tokenized_s_afinn_agg |>
  ggplot(aes(x = date_of_speech, y = sentiment_score)) +
  geom_point(alpha = 0.6) +
  geom_smooth() +
  labs(
    title = "Sentiment Scores of Hong Kong CE's Speeches and Articles"
  ) +
  xlab("Date") + ylab("Sentiment Scores (Normalized)")
```

## Calculate Scores of Emotions

```{r}
dict_nrc

d_tokenized_s_nrc = d_tokenized_s |>
  inner_join(dict_nrc, by = "word", multiple = "all")

d_tokenized_s_nrc_agg = d_tokenized_s_nrc |>
  group_by(uid, date_of_speech, sentiment) |>
  count() |>
  pivot_wider(names_from = "sentiment", values_from = "n", 
              names_prefix = "sentiment_score_")

names(d_tokenized_s_nrc_agg)

# Change of sentiment over time?
d_tokenized_s_nrc_agg |>
  ggplot(aes(x = date_of_speech, y = sentiment_score_sadness)) +
  geom_point(alpha = 0.6) +
  geom_smooth() +
  labs(
    title = "Sentiment Scores of Hong Kong CE's Speeches and Articles"
  ) +
  xlab("Date") + ylab("Sadness Scores")
```

# 4 Topic Modelling

## Introduction

This notebook demonstrate Topic Modeling

```{r}
library(tidyverse)
library(lubridate)
```

```{r}
d_fulltext = read_rds("Lecture_10/1_Text_Mining/data/data_ce_speech_article.rds")
# Change the date variable to "date" format
d_fulltext = d_fulltext |> mutate(date_of_speech = dmy(date_of_speech))
```

## Tokenization

```{r}
library(tidytext) # Full introduction: http://tidytextmining.com/
```

```{r}
d_tokenized = d_fulltext |>
  select(uid, date_of_speech, text) |>
  unnest_tokens(word, text)

head(d_tokenized, 20)

# Simple?
```

## Wrangling: Remove Stop Words

```{r}
# Load Stopwords
data("stop_words")

head(stop_words, 20)
```

```{r}
# Remove stopwords
d_tokenized_s = d_tokenized |>
  anti_join(stop_words, by = "word")
# anti_join: whatever appearing in the stop_words dataframe, we remove it.
```

## Wrangling [Optional]: Stemming

```{r}
if (!require(SnowballC)) install.packages("SnowballC")
library(SnowballC)
```

```{r}
d_tokenized_s = d_tokenized_s |>
  mutate(stem = wordStem(word))

head(d_tokenized_s, 20)
```

## Calculate Document-level Term Frequencies

```{r}
d_word_frequencies = d_tokenized_s |>
  group_by(uid, stem) |>
  count()

head(d_word_frequencies)
```

## Create Document-Term Matrix

```{r}
# library(topicmodels)
dtm = d_word_frequencies |> cast_dtm(uid, stem, n)

# What does a document-term matrix look like?
```

## Fit Topic Models

```{r}
# if (!require(topicmodels)) install.packages("topicmodels")
library(topicmodels)

# Set number of topics
K = 20

# Set random number generator seed
set.seed(1122)

# compute the LDA model, inference via 1000 iterations of Gibbs sampling
m_tm = LDA(dtm, K, method="Gibbs", 
            control=list(iter = 500, verbose = 25))
```

## Clean Results of Topic Models

```{r}
# install.packages("reshape2")
## beta: How words map to topics
sum_tm_beta = tidy(m_tm, matrix = "beta")

## gamma: How documents map on topics
sum_tm_gamma = tidy(m_tm, matrix = "gamma") |>
  rename("uid" = "document")

sum_tm_gamma_wide = sum_tm_gamma |>
  pivot_wider(names_from = "topic", values_from = "gamma", names_prefix = "topic_")
```

## Visualize Topic Modeling Results

```{r}
sum_tm_gamma |>
  group_by(topic) |>
  summarise(sum_gamma = sum(gamma)) |>
  arrange(desc(sum_gamma))
```

```{r}
TOP_N_WORD = 10

topic_top_word = sum_tm_beta |>
  rename("word" = "term") |>
  group_by(topic) |>
  slice_max(beta, n = TOP_N_WORD) |>
  arrange(topic, desc(beta))
```

```{r, fig.width=10, fig.height=10}
### Visualization 1: Topics in bar charts

topic_top_word |>
  mutate(word = reorder_within(word, beta, topic)) |>
  ggplot(aes(y = word, x = beta)) +
  geom_bar(stat = "identity") +
  facet_wrap(~topic, scales = "free_y") +
  scale_y_reordered() + # Very interesting function. Use with reorder_within
  labs(
    title = "Topic Modeling",
    subtitle = "Top words associated with each topic"
  )
```

```{r, fig.width=12, fig.height=12}
### Visualization 2: Topics in word cloud
library(ggwordcloud)

topic_top_word |>
  ggplot(aes(label = word, size = beta)) +
  geom_text_wordcloud() +
  scale_size_area(max_size = 8) + # Tune this number to change the scale
  facet_wrap(~factor(topic)) +
  labs(
   title = "Topic Modeling: Top words associated with each topic"
  ) +
  theme_minimal()
```

# Sub Section: Web Scraping

## 1.1 Retrieve Single doc

```{r}
library(tidyverse)
library(rvest) # This is one package that handles webpages retrieval and parsing
library(xml2) # This is another package that handles webpages retrieval and parsing

dir.create("Lecture_11/2_Web_Scraping/data_1") # Create a new folder

## Retrieve CE's last article ====

download.file(
  url = "https://www.ceo.gov.hk/archive/5-term/eng/pdf/article20220530.pdf",
  destfile = "Lecture_11/2_Web_Scraping/data_1/20220530.pdf")


## Retrieve CE's last speech ====

doc_html <- read_html("https://www.info.gov.hk/gia/general/202206/21/P2022062100598.htm")
write_html(doc_html, "Lecture_11/2_Web_Scraping/data_1/20220621.htm")

## read_html function is available on both rvest and xml2 packages
## write_html function is only available on xml2 package
```

## 1.2 Parse Single Doc

```{r}
library(tidyverse)
library(rvest)
library(xml2)

library(pdftools)
```

#### Parse a PDF document

```{r}
## Function to parse a PDF file
pdf_parsed <- pdf_text("Lecture_11/2_Web_Scraping/data_1/20220530.pdf")
pdf_parsed

## It returns a vector of 8 elements. Why 8? 8 pages! Each page is put in a 
## separate element.
length(pdf_parsed)

## Save the file into a .txt file (a text document)
write(pdf_parsed, file = "Lecture_11/2_Web_Scraping/data_1/20220530_parsed.txt")
```

#### Parse a webpage

```{r}
# Parse a webpage ====

## Load the HTML file
doc_html <- read_html("Lecture_11/2_Web_Scraping/data_1/20220621.htm")
typeof(doc_html)
class(doc_html)

## Check what it looks like again
print(doc_html)
```

#### Lazy parsing

```{r}
## Lazy parsing ====
## Just get all the text
html_parsed <- html_text(doc_html) # Blindly retrieve all the text form the webpage
print(html_parsed)
write(html_parsed, "Lecture_11/2_Web_Scraping/data_1/20220621_parsed_lazy.txt")
```

#### Precise parsing

```{r}
## Precise parsing ====

### Step 1: Use SelectGadget to locate the content of interest

### Step 2: Use R to locate the section
title <- doc_html |> html_node("#PRHeadlineSpan") |> html_text()

text_all <- doc_html %>%
  html_elements("#contentBody") %>%
  html_text()

text_title <- doc_html %>%
  html_elements("#PRHeadlineSpan") %>%
  html_text()

text_body <- doc_html %>%
  html_elements("#pressrelease") %>%
  html_text()
```

use Web Scraper app in google extension

#### Step 3: Save the results

```{r}
write(text_all, "Lecture_11/2_Web_Scraping/data_1/20220621_parsed_all.txt")
write(text_title, "Lecture_11/2_Web_Scraping/data_1/20220621_parsed_title.txt")
write(text_body, "Lecture_11/2_Web_Scraping/data_1/20220621_parsed_body.txt")
```
