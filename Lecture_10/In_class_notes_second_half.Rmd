---
title: "Lecture 10 in class notes (text mining secion)"
author: "Lau Wai Fung Raymond"
date: "2023-11-16"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1_String_Operation

## Introduction

In this note, I demonstrate how you may wrangle text data using the `stringr` element of Tidyverse.

## Load Packages

```{r, message=FALSE}
library(tidyverse) # Package for data work

library(knitr) # Package for data presentation at Rmarkdown
library(kableExtra)  # Package for data presentation at Rmarkdown
```

## Load Data

```{r, results="asis"}
d = read_rds("Lecture_10/1_Text_Mining/data/data_ce_speech_article.rds")

dim(d) # Get dimension of dataset d
names(d) # Get column names of dataset d

# str(d) # chr 
# DT::datatable(d |> slice(1:5))
```

## Getting to know strings

Strings, when referred to in R, should be moderated by quotation marks (single or double are equivalent).

```{r}
"This is a string"
"This is a string"

# A vector of string
c("String A", "String B", "String C")
```

Everything is the same when you moderate them by single quotation marks.

```{r}
'This is a string'
'This is a string'

a = 100
b = "this is a string"

# A vector of string
c('String A', "String B", 'String C')
```

String variables in a column are marked by variable types `chr` or `character`

```{r}
str(d)
summary(d)
class(d$date_of_speech)
typeof(d$date_of_speech)
```

## Case 1: When are the speeches/articles delivered?

Take a look at at the `date_of_speech` variable:

```{r}
d |> select(date_of_speech)
```

The `date_of_speech` variable is currently treated as a string and we are unable to get valuable information from it. Our first task is to use R's string operation to subtract inforamation of interest from this string-type indicator of date.

### Subtract strings using locations with `str_sub`

With start with the simplest string operation: Subtracting information of interest from a string based on locations.

```{r}
# String used.
?str_sub
```

#### Case: Getting YEAR

```{r}
d |>
  select(date_of_speech) |>
  # create a new variable called YEAR, that gets the  the last but three to the last characters of the string (minus mean start from the end)
  mutate(YEAR = str_sub(date_of_speech, 
                        start = -4, end = -1))
```

Note:

-   The key is the position

    -   `start = -4` means starting from the last but 4-1=3 position.

    -   `end = -1` means ending at the last position

-   If you take away the negative sign, then you are telling R to start/end at certain positions of the string. This will be useful for our extraction of the "day of month" information

#### Case: Getting DAY

```{r}
d |>
  select(date_of_speech) |>
  # DAY (of month): the first two characters of the strings
  mutate(DAY = str_sub(date_of_speech, start = 1, end = 2))
```

#### Exercise: How can you get MONTH?

Create a table named `d_date` containing the following variables: `filename` `date_of_speech` `YEAR` `MONTH` `DAY` . Use only `str_sub` for the string operation.

```{r, eval=FALSE}
d |>
  select(date_of_speech) |>
  mutate(DAY = str_sub(date_of_speech, start = 4, end = -5))
```

Intended output:

```{r, echo=FALSE, results="asis"}
# Answer
d_date = d |>
  select(filename, date_of_speech) |>
  mutate(
    YEAR = str_sub(date_of_speech, start = -4, end = -1),
    MONTH = str_sub(date_of_speech, start = 3, end = -5),
    DAY = str_sub(date_of_speech, start = 1, end = 2)
    )

head(d_date, 5) |> kable() |> kable_styling()
```

### Remove and replace

Up next, we discuss methods to remove certain patterins from a string or replace a pattern with another pattern. Specifically, we will discuss the following functions:

-   `str_remove` and `str_remove_all`

-   `str_replace` and `str_replace_all`

#### Case: Removing the `.` in the DAY variables with `str_remove`

```{r, echo=FALSE}
# Try this first
d_date |>
  mutate(DAY_t = str_remove(DAY, "."))
```

**Q:** What do you get? What is wrong?

**A:** In R (and many other programming languages, `.` is used as an indicator of "any random character." So the above is interpreted by R as "removing any character. This causes the error. The correct way to tell R you want to remove the `.` character is to write \``\\.` where the `\\` prefix tells R that you mean the character `.` instead of using it as an indicator of any character.

```{r}
d_date |>
  mutate(DAY_t = str_remove(DAY, "\\."))
```

#### Remove `.`'s in the `MONTH` variable using `str_remove_all`

Now, we want to do the same operation with `MONTH` . Let's start with the same `str_remove` variable we used above.

```{r}
d_date |>
  mutate(MONTH_t = str_remove(MONTH, "\\."), .after = MONTH)
```

**Q:** What do you get? What is wrong?

**A:** There are remaining `.` in the processed variable. This is because `str_remove` only removes the first match (from left to right). To remove all matches, you need `str_remove_all` .

```{r}
d_date |>
  mutate(MONTH_t = str_remove_all(MONTH, "\\."), .after = MONTH)
```

#### Exercise: Complete the cleaned data

Requirement:

-   Name the processed dataset `d_date_1`

-   Clean out the `.`'s

-   Convert `YEAR` `MONTH` `DAY` to numeric variables (hint: `as.numeric()`)

-   Optional: Provide summary statistics for `YEAR` `MONTH` `DAY` respectively

```{r}
d_date |>
  mutate(
    MONTH = str_remove_all(MONTH, "\\."),
    DAY = str_remove_all(DAY, "\\.")
  )

# Want to make it simpler? Use mutate_at
d_date |>
  mutate_at(vars(MONTH, DAY), ~str_remove_all(., "\\."))
```

### Replace patterns in strings

#### Case: Replace `.` by `-`

For demonstration purpose, let's do something meaningless first: Replace `.` by `-` for the `MONTH` variable.

```{r}
d_date |>
  mutate(MONTH = str_replace(MONTH, "\\.", "-"))
```

```{r}
d_date |>
  mutate(MONTH = str_replace_all(MONTH, "\\.", "-"))
```

Q: What are their differences? Can you tell the differences between `str_replace` and `str_replace_all`?

OK. The operation we have done in this step is meaningless. What is meaningful? Let's say, for formatting purpose, we want to do the below:

-   Remove the ending `.` and then

-   Replace the leading `.` with `0` .

-   For example, we want to convert `.5.` to `05` .

What should we do? This requires new knowledge: Regular Expression.

### Regular Expression

See Second page of <https://github.com/rstudio/cheatsheets/blob/main/strings.pdf>

Regular expression help you specify patterns of interest more effectively in strings.

-   How can we specify "ending `.` in `MONTH`?" Check the cheat sheet.

-   How can we specify "leading `.` in `MONTH`?" Check the cheat sheet.

```{r}
# Example: Removing the ending .
d_date |>
  mutate(MONTH_t = str_remove(MONTH, "\\.$"))

# Example: Removing the starting .
d_date |>
  mutate(MONTH_t = str_remove(MONTH, "^\\."))
```

A trick to see you have have found the right pattern in the string: `str_view`

```{r}
# Check the matched pattern in the first three entries of MONTH
# As an aside: "$" following dataframe's name select the variable; [1:3] selects the first three elements
str_view_all(d_date$MONTH[1:3], "\\.$", html = TRUE)

# This function will be more handy when your text is more complicated.
str_view_all(d$title[1:3], "Article by CE:", html = TRUE)
```

**Exercise**

1.  Replace the leading `.` of `MONTH` by 0
2.  Clean the `DAY` variable in the same way

```{r}
d_date |>
  mutate(
    MONTH = str_replace(MONTH, "^\\.", "0"),
    DAY = str_replace(DAY, "^\\.", "0")
  )
```

Check out the more complicated part of regular expression yourself (using the cheat sheet).

### Extract information from strings

What if we want to extract the numbers from dates? Use `str_extract`

```{r}
# Extract the first element found
d_date |>
  select(date_of_speech) |>
  mutate(date_of_speech_extract = 
           str_extract(date_of_speech, "[0-9]+"))

# Extract all the elements
d_date |>
  select(date_of_speech) |>
  mutate(date_of_speech_extract = str_extract_all(date_of_speech, "[0-9]+"))
```

Frequently seen output of string operation: A list column. We need to "unnest" list columns.

-   `unnest_wider` is typically used to unnest columns that have the same number of elements.

-   `unnest_longer` is typically used to unnest outputs that have various number of elements.

```{r}
# unnest_longer
# When unsure about the number of elements extracted. 
d_date |>
  select(date_of_speech) |>
  mutate(date_of_speech_extract = str_extract_all(date_of_speech, "[0-9]+")) |>
  unnest_longer(date_of_speech_extract)


# unnest_wider
d_date |>
  select(date_of_speech) |>
  mutate(date_of_speech_extract = str_extract_all(date_of_speech, "[0-9]+")) |>
  unnest_wider(date_of_speech_extract, names_sep = "_")
```

### Merge and Split Strings

Using the `str_` functions

-   Function that splits your strings using certain "separator": `str_split`

-   Function that merges your strings: `str_c`

```{r}
# Split the date variable using the separator
# Spearating the string using "." as spearators

# d_date |>
#   select(date_of_speech) |>
#   mutate(
#     date_of_speech_ex = str_split(date_of_speech, "\\.")
#   ) |> View()

d_date_result = d_date |>
  select(date_of_speech) |>
  mutate(
    date_of_speech_ex = str_split(date_of_speech, "\\.")
  ) |>
  unnest_wider(date_of_speech_ex, names_sep = "_")
```

```{r}
# Practice string merging with str_c
d_date_result

# Put together YEAR-MONTH-DAY

?str_c # paste, paste0

d_date_result |>
  mutate(
    # paste0
    date_merge = str_c(date_of_speech_ex_3, 
                       "-",
                       date_of_speech_ex_2, 
                       "-",
                       date_of_speech_ex_1
                       ))

d_date_result |>
  mutate(
    date_merge = str_c(date_of_speech_ex_3, 
                       "-",
                       str_pad(date_of_speech_ex_2, "0"), 
                       "-",
                       str_pad(date_of_speech_ex_1, "0")
                       ))
```

## Case 2: Wrangling the Title

Exercise:

-   Separate speeches and articles

-   Get speeches' locations

-   Identify policy addresses

-   Identify COVID-related speeches and article

```{r}
# Separate articles and speech

# Generate an variable indicating whether a piece is an article or a speech
d_2 = d |>
  mutate(
    article = str_extract(title, "Article by CE"),
    speech = str_extract(title, "Speech by CE"),
    .after = uid
  )

# Filter only articles
d |>
  filter(str_detect(title, "Article by CE"))
```

## Case 3: Wrangling the Main Text

Exercises:

-   Remove redundant spaces

-   Remove line breaks and tabulation (`\n` `\r` `\t`)

-   Get dates of speech

-   Remove procedural contents from speeches

-   Get all numbers mentioned

-   Get all money mentioned

# 2_Tokenization_Exploratory Analysis

## Introduction

This notebook demonstrate tokenization, basic text wrangling, and exploratory data analysis.

```{r}
library(tidyverse)
library(lubridate)
```

```{r}
d_fulltext = read_rds("Lecture_10/1_Text_Mining/data/data_ce_speech_article.rds")
# Change the date variable to "date" format
d_fulltext = d_fulltext |> mutate(date_of_speech = dmy(date_of_speech))
```

## Tokenization

```{r}
if (!require("tidytext")) install.packages("tidytext")

library(tidytext) # Full introduction: http://tidytextmining.com/
```

```{r}
d_fulltext <- d_fulltext |>
  mutate(text = str_replace_all(text, "Hong Kong", "HongKong"))

d_tokenized = d_fulltext |>
  select(uid, date_of_speech, text) |>
  unnest_tokens(word, text)
# first arg: output name; second: input name

head(d_tokenized, 20)

# Simple?
```

## Wrangling: Remove Stop Words

```{r}
# Load Stopwords
data("stop_words")

head(stop_words, 20)
```

```{r}
# Remove stopwords
d_tokenized_s = d_tokenized |>
  anti_join(stop_words, by = "word")
# anti_join: whatever appearing in the stop_words dataframe, we remove it.
```

## Wrangling [Optional]: Stemming

```{r}
if (!require(SnowballC)) install.packages("SnowballC")
library(SnowballC)
```

```{r}
d_tokenized_s = d_tokenized_s |>
  mutate(stem = wordStem(word))

head(d_tokenized_s, 20)
```

```{r}
# Stemming reduces the number of unique words
d_tokenized_s$word |> unique() |> length()
d_tokenized_s$stem |> unique() |> length()
```

## Exploratory Data Analysis

### Count word frequencies

```{r}
# Count term frequencies (for raw words)
word_frequency = d_tokenized_s |>
  count(word, sort = TRUE)

head(word_frequency, 20)

# Equivalent operation
word_frequency <- d_tokenized_s |>
  group_by(word) |>
  summarise(n = n())

# Remove numbers
# word_frequency <- word_frequency |>
#  filter(!str_detect(word, "[0-9]+"))

# Count term frequencies (for Stemmed word -- recommended)
word_frequency = d_tokenized_s |>
  count(stem, sort = TRUE) |>
  rename("word" = "stem")

head(word_frequency, 20)
```

### Examine most popular words

```{r}
# Get a subset of most frequent words
word_frequency_top = word_frequency |>
  arrange(desc(n)) |> # Make sure that it is sorted properly
  slice(1:200) # Take the first 200 rows. 
```

### Plot most popular words

```{r}
word_frequency_top |>
  slice(1:10) |>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(x = n, y = word)) +
  geom_col() +
  theme_bw()
```

### Plot a Word Cloud

```{r}
if (!require(ggwordcloud)) install.packages("ggwordcloud")
library(ggwordcloud)

word_frequency_top |>
  slice(1:100) |>
  ggplot(aes(label = word, size = n)) +
  scale_size_area(max_size = 14) +
  geom_text_wordcloud() +
  theme_minimal()
```

```{r}
# An alternative wordcloud package
if (!require(wordcloud)) install.packages("wordcloud")
library(wordcloud)

wordcloud(
  word_frequency_top$word, word_frequency_top$n, 
  rot.per = 0, random.order = FALSE, random.color = TRUE)


wordcloud(
  word_frequency_top$word, word_frequency_top$n, 
  rot.per = 0.2, random.order = FALSE, random.color = TRUE)
```

```{r, results='hide'}
# The third wordcloud package
# https://r-graph-gallery.com/196-the-wordcloud2-library.html
if (!require(wordcloud2)) install.packages("wordcloud2")
library(wordcloud2)

wordcloud2(word_frequency_top)

wordcloud2(word_frequency_top, shape = "star")

wordcloud2(word_frequency_top, shape = "pentagon")
```

## Comparative Exploratory Analysis

How does the focus differ between 2021 and 2020? Our final set of analysis in this note focuses on a comparative analysis of word frequencies.

```{r}
# Calculate term frequencies for 2020 and 2021 respectively
word_frequency_compare_21_20 = 
  d_tokenized_s |>
  mutate(year = year(date_of_speech), .after = "date_of_speech") |>
  # Extract the year of the speech
  filter(year == 2020 | year == 2021) |>
  group_by(year, stem) |>
  count(sort = TRUE) |>
  pivot_wider(names_from = "year", values_from = "n", 
              names_prefix = "n_", values_fill = 0) |>
  ungroup() |>
  mutate(
    prop_2021 = n_2021 / sum(n_2021),
    prop_2020 = n_2020 / sum(n_2020)
  )
```

```{r}
# Visualize the word frequencies in the two years
word_frequency_compare_21_20 |>
  ggplot(aes(x = prop_2020, y = prop_2021)) +
  geom_point()

word_frequency_compare_21_20 |>
  ggplot(aes(x = prop_2020, y = prop_2021)) +
  geom_point() +
  scale_x_sqrt() + scale_y_sqrt()


word_frequency_compare_21_20 |>
  ggplot(aes(x = log(prop_2020), y = log(prop_2021))) +
  geom_point()

word_frequency_compare_21_20 |>
  filter(n_2020 >= 10) |>
  ggplot(aes(x = log(prop_2020), y = log(prop_2021))) +
  geom_point() +
  geom_smooth()
```

```{r}
# The biggest difference?

## What are the words that feature 2020 speeches
tmp_plot_20 = word_frequency_compare_21_20 |>
  mutate(diff = prop_2020 - prop_2021) |>
  slice_max(diff, n = 30) |>
  arrange(desc(diff))
  
## What are the words that feature 2021 speeches
tmp_plot_21 = word_frequency_compare_21_20 |>
  mutate(diff = prop_2021 - prop_2020) |>
  slice_max(diff, n = 30) |>
  arrange(desc(diff))
```

```{r}
# Visualize the difference in a nice way?
set.seed(327)
tmp_plot_merge = tmp_plot_21 |> 
  mutate(Year = "2021") |>
  bind_rows(
    tmp_plot_20 |> mutate(Year = "2020")
    ) 

tmp_plot_merge |>
  ggplot(aes(label = stem, x = Year, color = Year, size = abs(diff))) +
  geom_text_wordcloud_area() +
  scale_size_area(max_size = 14) +
  theme_minimal() +
  theme(legend.position = "top")

tmp_plot_merge |>
  ggplot(aes(label = stem, y = Year, color = Year, size = abs(diff))) +
  geom_text_wordcloud_area() +
  scale_size_area(max_size = 14) +
  theme_minimal() +
  theme(legend.position = "top")
```

# 3_Sentiment_Analysis

**continue in next lecture 11..**
